{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92535b4c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Bayesian Inference and the Logic of Discovery\n",
    "\n",
    "##  2024 University of Sydney Hunstead Lecture II\n",
    "### Bryan Scott, CIERA | Northwestern University\n",
    "Based on \n",
    "- Lectures by David Hunter and Hyungsuk Tak given at the 14th Penn State School in Astrostatistics\n",
    "- Probability Theory: The Logic of Science by ET Jaynes\n",
    "- Intro to Probability and Priors, Likelihoods, and All That, LSST DA Data Science Fellowship Program Session 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80f068a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's start with the basic intuition: Rolls of the Dice\n",
    "\n",
    "<img src=\"D20.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b4032e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 1: Let's define some jargon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b214c0d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### We define the following terms:\n",
    "\n",
    "- The **Outcome space**, denoted $\\Omega$  is the set of possible **outcomes** of some process. We can write this as,\n",
    "\n",
    "$\\Omega = \\{o_1, o_2, ... o_n\\}$\n",
    "\n",
    "So, for example, the outcome space for a coin is {H, T}, and for a 6-sided die, {1, 2, 3, 4, 5, 6}.\n",
    "\n",
    "- An **Event** is a subset of the Sample Space. $E \\in \\Omega$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e23200",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Technical note: A **discrete sample space** is either **finite** or **countably infinite**. This definition helps with the formal mapping between outcomes in the sample space and the notion of probability. In astronomical applications, we largely ignore the technical issues that come with whether our outcome spaces are discrete or continuous (\"not countably infinite\"). In this case we need a more sophisticated notion of a **probability space**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dafd25c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## An Important and Confusing Term: The \"Random Variable\"\n",
    "\n",
    "A **random variable** is a **map** from the **outcome space** to the **real numbers**. There are a few notations for a random variable, for example:\n",
    "\n",
    "- Formally, $X: \\Omega \\rightarrow R$ defines the map from the outcome space to (a subset?) of the real numbers \n",
    "\n",
    "- If we want to consider a specific sort of event, we write $\\{\\omega \\in \\Omega: x \\in X\\}$\n",
    "\n",
    "- The short hand for the above is $\\{X = x\\}$\n",
    "\n",
    "Careful, the random variable X is the **map**, not the specific **outcome or event**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d86dc6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"outcomespace.png\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d69b1ab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Probabilities and Random Variables\n",
    "\n",
    "The **probability mass function** associated with a **random variable** is a map between the elements of the outcome space, the **events**, and real numbers. It is written in shorthand as P(X = x) and more verbosely as, \n",
    "\n",
    "$P(\\{\\omega \\in \\Omega: x \\in X\\} = P(\\{\\omega \\in \\{H, T\\}: X(\\omega)) = T\\})$ = 1/2\n",
    "\n",
    "for the example of example of flipping a coin and getting tails is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fed93d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## How do we assign probabilities?\n",
    "\n",
    "Take a moment and discuss this with those around you. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d78cff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Principle of \"Insufficient Reason\" or \"Indifference\"\n",
    "\n",
    "First formulated by Laplace and Bernoulli, Keynes (en route to criticizing it) defined this principle as:\n",
    "\n",
    "\"If there is no known reason for predicating of our subject one rather than another of several alternatives, then relatively to such knowledge the assertions of each of these alternatives have an equal probability.\"\n",
    "\n",
    "So if I flip a coin, I assign uniform probability to all outcomes given by N/M (for N identical outcomes out of M possibilities) = 1/2 for {H, T}, 1/6 for flipping a coin {1, 2, 3, 4, 5, 6}. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac54fb32",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Functions of random variables\n",
    "\n",
    "Fact: Any function of a random variable is itself a random variable. \n",
    "\n",
    "Problem: We often measure some variable x, but the result we are interested n is a function y(x). What is the distribution P(y)? If y = $\\Phi(x)$ and hence $x = \\Phi^{-1}(y)$,\n",
    "\n",
    "$$ p(y) = p[\\Phi^{-1}(y)] \\left|\\frac{d \\Phi^{-1}(y)}{d y} \\right|$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e10f233",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Some important remarks: \n",
    "\n",
    "* Cumulative statistics are invariant under monotonic transformations (they map to the same data point) - this provides the basis for a number of statistical tests that compare distributions. \n",
    "\n",
    "* The standard uncertainty propogation formulas are derived by a taylor expansion of the uncertainty to first order, \n",
    "\n",
    "$$ \\sigma_y = \\left| \\frac{d \\Phi(x)}{dx} \\right| \\sigma_x $$\n",
    "\n",
    "Careful: these formulas only work if it is sufficient to keep only the first order terms in the transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb5d73e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 2: Kolgomorov Axioms of Probability Theory\n",
    "\n",
    "There are three basic axioms that restrict the form that P can take:\n",
    "\n",
    "$$P(\\omega) \\ge 0 \\space \\forall \\space \\omega \\in \\Omega$$ (probabilities are never negative)\n",
    "\n",
    "$$\\Sigma_i P(\\omega_i) = 1$$ \n",
    "\n",
    "if the $\\omega_i$ span the entire outcome space. (probabilities must sum to 1)\n",
    "\n",
    "$$P(\\cup_i^\\infty \\omega_i) = \\Sigma P(\\omega_i)$$ \n",
    "\n",
    "for disjoint $\\omega_i$ (countable additivity or \"the sum rule\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1795470",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 3: A Quick Proof of the Bayes' rule: Conditional Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc516893",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Suppose you have some event, which we will call $A$. We define the probability of event $A$ occurring as:\n",
    "\n",
    "$$P(A).$$\n",
    "\n",
    "Now suppose we want to know the probability that both event $A$ and event $B$ occur: $P(A \\cap B)$. At first glance, it seems like this ought to be the product of the probability of $A$ and the probability of $B$:\n",
    "\n",
    "$$P(A \\cap B) = P(A)\\,P(B).$$\n",
    "\n",
    "This is the product rule if $A$ and $B$ are *independent*. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5220c18a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To see why this is true, imagine a single coin. If event $A$ is a flip landing in heads, and event $B$ is a flip landing in tails, then $P(A)\\,P(B) = 1/4$. \n",
    "\n",
    "What if P(A) depends on the P(B)?\n",
    "\n",
    "In that case, the probability of $A$ *and* $B$ therefore requires a statement about conditional probability:\n",
    "\n",
    "$$P(A \\cap B) = P(A\\mid{B})\\,P(B),$$\n",
    "\n",
    "which should be read as \"the probability of $A$ and $B$ is equal to the probability of $A$ given $B$ multiplied by the probability of $B$.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5744a8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Product Rule\n",
    "\n",
    "<img src=\"product_rule.jpeg\" alt=\"drawing\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e2d5af",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The probability of $A$ and $B$ must be equal to the probability of $B$ and $A$, which leads to:\n",
    "\n",
    "$$P(A\\mid{B})\\,P(B) = P(B\\mid{A})\\,P(A),$$\n",
    "\n",
    "which we can rearrange as:\n",
    "\n",
    "$$P(A\\mid{B}) = \\frac{P(B\\mid{A})\\,P(A)}{P(B)}.$$ (This is the Bayes' Rule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968cca3c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conditionalization and the Law of Total Probability\n",
    "\n",
    "First we define the concept of a **partition** of a set. A partition is a set of disjoint sets whose unions is the outcome space, $\\Omega$.\n",
    "\n",
    "Then, the law of total probability says that the probability of an event, A, can be found by summing over all of the ways A and events in the partition of $\\Omega$ can occur, mathematically,\n",
    "\n",
    "$$ P(A) = \\Sigma_i^N P(A \\cap B_i) $$\n",
    "\n",
    "The definition of $P(A \\cap B)$ allows us to write,\n",
    "\n",
    "$$ P(A) = \\Sigma_i^N P(A|B_i)P(B_i) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab78598",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Example of the Law of Total Probability:  \n",
    "\n",
    "$$ P(\\text{H on four flips}) = P(H|\\text{not trick}) \\times P(\\text{not trick}) + P(H |\\text{trick}) \\times P(\\text{trick}) $$\n",
    "\n",
    "$$ P(\\text{H on four flips}) = \\left(\\frac{1}{2}\\right)^4 \\times \\frac{5}{6} + 1 \\times \\frac{1}{6} = \\frac{7}{32}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a308c7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 4: Relationship Between Probability and Inference\n",
    "\n",
    "As scientists, do we care about this probability? I would argue we are much more interested in the idea of **explanation**, which is what the Bayes' rule now allows us to attempt. We will **condition** our explanation on the **data** as follows:\n",
    "\n",
    "$$ P(\\text{trick} | \\text{H on four flips}) = \\frac{P(\\text{H on four flips}| \\text{trick})P(\\text{trick})}{P(\\text{H on four flips})} = \\frac{16}{21}$$\n",
    "\n",
    "which captures are intuition that, if we think the coin is rigged and the flips don't go our way, that we're probably being cheated! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab35195",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This example illustrates the relationship between probability and inference. \n",
    "\n",
    "$$ \\text{Roughly: Probability explains how likely various outcomes (observations) are, given the model parameter }  \\theta, \\text{while inference quantifies the uncertainty about } \\theta\\text{, given observed data x.} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2a540b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In classical statistics, we think of the problems we encounter in the following way: \n",
    "\n",
    "There exists a **population** from which we **sample** (select subsets of). We describe the sample with sets of descriptive **statistics**, for example, the sample **mean**, the sample **variance**, the sample **skeweness**, the sample **kurtosis**, etc.\n",
    "\n",
    "We then use the **sample statistics** to do inference, that is, to estimate, or infer, the parameters of the unobserved **population probability distribution**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5d2346",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 5: What is Probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87110992",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you review this lecture, you'll notice something disturbing. I haven't defined precisely what probability is. This is because there is, in fact, no consensus interpretation (beyond the notion of maps and the Kolgomorov axioms - even then, there are other probability axioms). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca529a29",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The basic debate comes down to the status of the parameters $\\theta$. There are two perspectives: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58b7bac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The $\\theta$ are fixed parameters to be estimated from (possibly) many repeated samples of the population. The sampling or the realization of the random process is the source of randomness. Our **estimates** have an associated probability distribution. Probability is thought of in terms of the long run frequency of events. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ddfe8f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The data are fixed - produced by some underlying physical process. $\\theta$ is some random variable with associated probability distributions $p(\\theta)$ and $p(\\theta | D)$. Probability is a measure of our uncertainty or beliefs about $\\theta$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2628709c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The former interpretation is often called the classical or frequentist interpretation, owing to its focus on the notion of repeated sampling. The latter is called the Bayesian interpretation, after the Rev. Thomas Bayes, who first argued for it in a posthumous essay. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57e13d1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Although you are (almost always) free to work within either interpretation, the dominant view in contemporary astronomy is a Bayesian one. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91fdd815",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Why? Because we only have one universe. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20174b16",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 6: Bayesian Inference\n",
    "\n",
    "Up until now, the following relation has just been a statement about conditional probabilities. We can take our \"Bayesian interpretation\" seriously and swap A and B for more suggestive names, H for hypothesis, D for data (and we'll include I for background information). Then the Bayes rule tells us how to update our beliefs about a hypothesis based on observing data. \n",
    "\n",
    "$$ P(H|D, I) = \\frac{P(D|H,I)P(H|I)}{P(D|I)}$$\n",
    "\n",
    "The different terms have special names. \n",
    "\n",
    "* P(H|D, I) is called the $\\textbf{posterior}$\n",
    "* P(D|H, I) is called the $\\textbf{likelihood}$\n",
    "* P(H| I) is called the $\\textbf{prior}$\n",
    "* P(D| I) is called the $\\textbf{evidence}$ or sometimes the $\\textbf{fully marginalized likelihood}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34988a0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As scientists, our goal is to construct models of the world based on what we observe. So our goal is to calculate posterior probabilities that our theories are true. \n",
    "\n",
    "Calculating the evidence is a difficult computational task and involved in the problem of model comparison. However, if we have one model for the data and want to estimate its parameters, we can ignore the bayesian evidence and simply normalize our posterior probability distributions after we are done. \n",
    "\n",
    "That leaves us with the (possibly) significant problem of determining the functional form of the likelihood and assigning priors to our model parameters. \n",
    "\n",
    "The most common distribution in both Bayesian (and classical inference) is the Gaussian. So let's take a minute and try to understand why it arises so often.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2881370",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 7: Why Gaussian Likelihoods? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964af5df",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The Herschelâ€“Maxwell derivation\n",
    "\n",
    "Many of the earliest clear derivations of the Gaussian distribution come from astronomy. In fact, much of statistical inference was concerned with fitting the orbits of planets and positions of stars. William Herschel considered the problem, specifically, of estimating the position of a star given noise in both the x and y directions (we'll assume some sort of \"flat sky\" approximation). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c0044b",
   "metadata": {},
   "source": [
    "Herschel argued that if the uncertainties in the stellar positions are independent of where on the sky you're measuring, two things must be true: \n",
    "\n",
    "* P1) The error in the x- and the error in the y- direction must be independent. \n",
    "* P2) The probability of the star having some x,y position is independent of the angle between x and y.\n",
    "\n",
    "P1 implies that the probability distribution for the x-y position factors,\n",
    "\n",
    "$$ \\rho(x, y) dx dy = f(x) dx \\times f(y) dy $$\n",
    "\n",
    "if we write this in polar coordinates $(r, \\theta)$, \n",
    "\n",
    "$$ \\rho(x, y) dx dy = g(r, \\theta) r dr d\\theta $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039aac45",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "P2 implies that this probability distribution is solely a function of $r = \\sqrt{x^2 + y^2}$\n",
    "\n",
    "$$ f(x) f(y) = f\\left(\\sqrt{x^2 + y^2}\\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df81841",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's consider the x- and y- parts separately by setting y = 0 and take the log of both sides. Then we have a functional equation, \n",
    "\n",
    "$$ \\log{\\left[\\frac{f(x)}{f(y=0)}\\right]} + \\log{\\left[\\frac{f(y)}{f(y=0)}\\right]} = \\log{\\left[\\frac{f\\left(\\sqrt{x^2 + y^2}\\right)}{f(y=0)}\\right]} $$\n",
    "\n",
    "for the left and right sides to be equal, f(x) = $\\alpha x^2$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60964be",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Since an identical argument works in the y-direction, we arrive at \n",
    "\n",
    "$$ \\rho(x,y) = \\frac{\\alpha}{\\pi} \\exp{\\left(-\\alpha (x^2 + y^2)\\right)} $$\n",
    "\n",
    "Maxwell used a similar argument to show that the 3D velocity distribution of a classical gas is Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b888cb1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The point here is that highly symmetric problems (for example those possessing rotational invariance) give rise to Gaussian distributions under very general conditions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32764535",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A more common argument for the Gaussian distribution as a very general choice for the likelihood is due to Landon.\n",
    "\n",
    "This argument is the distribution of electrical noise voltages. The key idea is that a sort of invariance in this distribution uniquely fixes the functional form of the noise voltage probability distribution. \n",
    "\n",
    "Suppose that the distribution of voltages in a circuit at a set of times is not given by a single fixed distribution, but a hierarchy of distributions,\n",
    "\n",
    "$$ p(\\nu | \\sigma^2) $$ \n",
    "\n",
    "where $\\sigma^2$ is the square of the noise voltage. If increasing the voltage in the circuit also increases the noise level, Landon posited that the distribution would have the same functional form just at a new location in the distribution hierarchy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff122f71",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Adding small increments through successive processes is very common in nature (think collisions in a classical gas) and can produce gaussian distributions. The intuition that such successive convolutions of probability distributions in a hierarchy can generate \"special\" distributions is the takeaway."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27130e4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 8a: When does the choice of Prior matter? \n",
    "\n",
    "### Small data limit\n",
    "\n",
    "<center>\n",
    "<img src=\"priors.png\" alt=\"drawing\" width=\"500\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bb4998",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Concavity/shape\n",
    "\n",
    "The other time that your choice of prior can matter a great deal is if the prior function modulates the posterior near the maximum of the likelihood function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c27ea4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Part 8b: Uniform \"uniformative\" Priors\n",
    "\n",
    "### Flat priors\n",
    "\n",
    "The $\\textbf{principle of consistency}$ says that the prior distribution should not change because of a transformation of the underlying coordinates. That is, if I move everything uniformly, my prior probabilities should remain the same in the new coordinates. This yields a $\\textbf{flat prior}$, \n",
    "\n",
    "$$ p(\\theta | I) = \\theta. $$ \n",
    "\n",
    "While a flat prior is intuitive, it has a problem - transformations of $\\sigma$ aren't shape preserving. Compare this with yesterday's discussion of the $\\textbf{principle of indifference}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1b95bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Scale Invariant Priors\n",
    "\n",
    "The same prior information can yield different prior distributions based on different principles. For example, if we replace the $\\textbf{principle of consistency}$ with a $\\textbf{symmetry principle}$, which states that a scale parameter should be unit independent, the prior distribution is,\n",
    "\n",
    "$$ p(\\theta | I) = \\theta^{-1}. $$\n",
    "\n",
    "which is equivalent to a flat prior in $\\log{\\theta}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0518c3b5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Principle of Maximum Entropy\n",
    "\n",
    "Some facts:\n",
    "\n",
    "* If we have a fixed mean and variance for our prior distribution, something called the principle of maximum entropy yields the Gaussian distribution as the best prior that respects our existing knowledge fo the problem.\n",
    "* If we have an upper or lower bound on a parameter (for example, some things can't be negative), the maximum entropy principle yields the exponential distribution.\n",
    "\n",
    "The maximum entropy principle can be used to derive other prior probability distributions in situations where we are not entirely ignorant about a problem. This is all deeply intertwined with the formulation of classical (Shannon) information theory. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e10de9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "That is (unfortunately) best left as the subject of another lecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6c2fab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusions:\n",
    "\n",
    "Interpreting the Bayes relation as a statement about uncertainty or belief gives rise to a model for scientific inference that allows us to solve real problems in a principled way. \n",
    "\n",
    "The cost of this is that we must now worry about the effect our beliefs have before we take data. We probably should have worried about this before we became Bayesians too but in any case, we can not escape having to choose a prior. A flat prior in one parameterization will be a decidedly not flat prior in another parameterization. \n",
    "\n",
    "The tutorial notebook includes a classic example where this matters in astronomy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9776106",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Questions?"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
